{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# erg 文件处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  将指定文件夹下的所有 erg 文件合并为一个 csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将 `path` 修改为 erg 文件所在的路径\n",
    "  - 注意不要去掉示例中字符串 `r\"D:\\工作\\2020\\3月\\UAES_C1.1_SPLIT\\torque characteristic\\Motor\\400V\" ` 前的 `r`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***\n",
    "# 需要修改的参数\n",
    "path = r\"D:\\工作\\2020\\3月\\UAES_C1.1_SPLIT\\torque characteristic\\Motor\\400V\"   # 文件夹路径\n",
    "# ***\n",
    "\n",
    "from file_operator import FileOperator\n",
    "import os\n",
    "\n",
    "file_operator = FileOperator(path)\n",
    "file_operator.handle_ergs(merge=True, file_name=os.path.join(path, path.split(os.path.sep)[-1] + '_merge_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  将指定文件夹下的所有 erg 文件转换为单独的 csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将 `path` 修改为 erg 文件所在的路径\n",
    "  - 注意不要去掉示例中字符串 `r\"D:\\工作\\2020\\3月\\UAES_C1.1_SPLIT\\torque characteristic\\Motor\\400V\" ` 前的 `r`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***\n",
    "# 需要修改的参数\n",
    "path = r\"D:\\工作\\2020\\3月\\UAES_C1.1_SPLIT\\eff\\400V\"   # 文件夹路径\n",
    "# ***\n",
    "\n",
    "from file_operator import FileOperator\n",
    "import os\n",
    "\n",
    "file_operator = FileOperator(path)\n",
    "file_operator.handle_ergs(merge=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将指定的 erg 文件转换为 csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将 file_name 修改为要转换的 erg 文件\n",
    "  - 注意不要去掉示例中字符串 `r\"D:\\工作\\2020\\3月\\UAES_C1.1_SPLIT\\torque characteristic\\Motor\\400V\\60s.erg\" ` 前的 `r`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = r\"D:\\工作\\2020\\3月\\UAES_C1.1_SPLIT\\torque characteristic\\Motor\\400V\\60s.erg\"   # erg 文件夹名称\n",
    "\n",
    "from file_operator import FileOperator\n",
    "import os\n",
    "\n",
    "file_operator = FileOperator(file_name)\n",
    "file_operator.handle_ergs(merge=False, file_name=''.join([os.path.splitext(file_name)[0], '.csv']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 按工况点求均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目前只支持 csv 文件求取平均值\n",
    "- file_name 参数\n",
    "  - 如果为文件夹，则先会将此文件夹下的所有 csv 文件合并后，再按工况点求均值\n",
    "  - 如果为文件，则只会对当前文件按工况点求均值\n",
    "- `flag` 参数\n",
    "  - 工况点标志位，需要根据实际情况修改\n",
    "- 'handle_error_data'\n",
    "  - 如果为 True， 则会按照 PA_signal < 1e10, 对数据进行筛选\n",
    "  - 如果为 False， 则不会对数据进行筛选\n",
    "  - 目的是为了防止出现假值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***\n",
    "# 需要修改的参数\n",
    "\n",
    "file_name = r\"D:\\工作\\2020\\3月\\UAES_C1.1_SPLIT\\eff\\400V\"\n",
    "flag='speed_step'\n",
    "handle_error_data = False\n",
    "PA_signal = 'PA1_PM [W]'\n",
    "\n",
    "# ***\n",
    "\n",
    "from file_operator import FileOperator\n",
    "import os\n",
    "\n",
    "file_operator = FileOperator(file_name)\n",
    "file_operator.get_average(flag=flag, handle_error_data=handle_error_data, PA_signal=PA_signal, file_name=None, head=True, line_count=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 递归的处理文件夹中的所有 erg 文件并求平均值 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将 `path` 修改为 erg 文件所在的路径\n",
    "  - 注意不要去掉示例中字符串 `r\"D:\\工作\\2020\\3月\\UAES_C1.1_SPLIT\" ` 前的 `r`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\工作\\2020\\3月\\UAES_C1.1_SPLIT\"\n",
    "\n",
    "head = False  # 是否取前 n 个点\n",
    "line_count = None  # \n",
    "\n",
    "\n",
    "import os\n",
    "from file_operator import FileOperator\n",
    "\n",
    "for root, dirs, _ in os.walk(path):\n",
    "    for directory in dirs:\n",
    "        sub_path = os.path.join(root, directory)\n",
    "        _file_name =  \"_\".join(sub_path.split(os.path.sep)[len( path.split(os.path.sep) ):])\n",
    "        result_dir = os.sep.join(path.split(os.sep)[:-1] + [path.split(os.sep)[-1] + 'result']\n",
    "                + sub_path.split(os.path.sep)[len(path.split(os.path.sep) ):])\n",
    "        file_operator = FileOperator(sub_path, result_dir)\n",
    "        if file_operator.erg_files:\n",
    "            file_name = os.path.join(result_dir,_file_name+\".csv\")\n",
    "            file_operator.handle_ergs(merge=True, file_name=file_name)\n",
    "            new_file_operator = FileOperator(result_dir)\n",
    "            file_name = os.path.join(result_dir,\"Average_\"+_file_name+\".csv\")\n",
    "            new_file_operator.get_average(file_name=file_name, head=head, line_count=line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转换 DIAdem (.dat) 至 csv 文件 （测试）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data_file 参数\n",
    "  - 需要修改为 dat 文件，需要是文件，暂不支持目录\n",
    " - 由于 excel 会限制打开文件时显示的行数，因此当记录超过 $2^{20}$ 后，会分为多个 csv 文件进行储存，确保单个 csv 文件的行数不超过 $2^{20}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***\n",
    "# 需要修改的参数\n",
    "dat_file = r\"C:\\Users\\lijunjie3\\Desktop\\tmp\\tmp\\10℃\\10℃-cycle3\\measure 100hzdat base+uaes-c1.2-gdv-eef+data191226+time195708+n_cycle\\measure 100hzdat base+uaes-c1.2-gdv-eef+data191226+time195708+n_cycle.dat\"  # 换为自己的文件\n",
    "# ***\n",
    "\n",
    "import os\n",
    "from collections import namedtuple, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "# 处理 data， 获取 head 信息\n",
    "Channel = namedtuple('Channel', 'name, unit, channel_type,file_name, method, data_type, nums, start_index, block_offset')\n",
    "\n",
    "with open(dat_file, encoding='cp1258') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "\n",
    "p = r\"#BEGINCHANNELHEADER\\n(.+?)#ENDCHANNELHEADER\\n\"\n",
    "import re\n",
    "channel_contents = re.findall(p, text, re.DOTALL)\n",
    "\n",
    "if not channel_contents:\n",
    "    with open(dat_file, encoding='utf16') as f:\n",
    "        text = f.read()\n",
    "        for line in f:\n",
    "            print(line)\n",
    "    channel_contents = re.findall(p, text, re.DOTALL)\n",
    "\n",
    "channels = []\n",
    "for channel_content in channel_contents:\n",
    "    contents = channel_content.split('\\n')\n",
    "    dic = {}\n",
    "    for content in contents:\n",
    "        l = content.split(',')\n",
    "        if len(l)>=2:\n",
    "            dic[int(l[0])] = l[1]\n",
    "    if dic.get(213) == 'BLOCK':\n",
    "        channels.append(Channel(dic.get(200), dic.get(202), dic.get(210), dic.get(211), dic.get(213), dic.get(214), \n",
    "                                int(dic.get(220)),int(dic.get(221)), int(dic.get(222))))\n",
    "    else:\n",
    "        channels.append(Channel(dic.get(200), dic.get(202), dic.get(210), dic.get(211), dic.get(213), dic.get(214), int(dic.get(220)), int(dic.get(221)), None))\n",
    "\n",
    "\n",
    "\n",
    "# 获取数据文件\n",
    "datas = {}\n",
    "datatypes = {'REAL32': 'float32', 'REAL64':'float64', 'INT16': 'int16', 'INT32':'int32'}\n",
    "channel_datas = {}\n",
    "for channel in channels:\n",
    "    if channel.channel_type != 'EXPLICIT':\n",
    "        raise Exception(\"Channel type {} is not supported\".format(channel.channel_type))\n",
    "        \n",
    "    if channel.data_type not in datatypes:\n",
    "        raise TypeError(\"Data Type {} is not supported in this version.\".format(channel.data_type))\n",
    "        \n",
    "    if channel.file_name not in datas:\n",
    "        datas[channel.file_name] = np.fromfile(os.path.join(os.path.split(dat_file)[0], channel.file_name), dtype=datatypes[channel.data_type])\n",
    "        \n",
    "    if channel.unit is not None:\n",
    "        channel_name = \"{} [{}]\".format(channel.name, channel.unit)\n",
    "    else:\n",
    "        channel_name = channel.name\n",
    "        \n",
    "    if channel.method == 'CHANNEL':\n",
    "        channel_datas[channel_name] = datas[channel.file_name][channel.start_index-1:channel.start_index + channel.nums - 1]\n",
    "    elif channel.method == 'BLOCK':\n",
    "        channel_datas[channel_name] = datas[channel.file_name][channel.start_index-1::channel.block_offset]\n",
    "        \n",
    "\n",
    "df = pd.DataFrame(channel_datas)\n",
    "number_of_chunks = math.ceil(len(df) / 2** 20)\n",
    "for id, df_i in enumerate(np.array_split(df, number_of_chunks)):\n",
    "    df_i.to_csv('{}_{}.csv'.format(os.path.splitext(dat_file)[0], id+1), index=0, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
